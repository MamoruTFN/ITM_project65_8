{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,img_to_array\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50, DenseNet121\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D,Convolution2D,BatchNormalization\n",
    "from tensorflow.keras.layers import Flatten,MaxPooling2D,Dropout\n",
    "\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,img_to_array\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "NUM_CLASSES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = r'C:\\Users\\thanaphat turienngam\\Desktop\\Cassava\\classification\\dataset_aug\\trainwithaugment'  #ใส่ path Data ที่มี Folder train, test \n",
    "Dataname = \"ByLT(deleteData)\"  #ใส่ชื่อไว้แสดงเฉยๆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "data_dir = DATASET_PATH\n",
    "data_dir = pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4200\n"
     ]
    }
   ],
   "source": [
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4200 files belonging to 4 classes.\n",
      "Using 3360 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=725,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,label_mode='categorical',\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4200 files belonging to 4 classes.\n",
      "Using 840 files for validation.\n"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=725,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,label_mode='categorical',\n",
    "    shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cbb', 'cbsd', 'cmd', 'healthy']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=tf.keras.applications.DenseNet121(\n",
    "#     include_top=True,\n",
    "#     weights=None,\n",
    "#     input_tensor=None,\n",
    "#     input_shape=(img_height, img_height, 3),\n",
    "#     pooling=None,\n",
    "#     classes=5,\n",
    "#     classifier_activation=\"softmax\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"DenseNet121\"\n",
    "# optimizer = \"Adam\"\n",
    "\n",
    "# model_base=DenseNet121(\n",
    "#     include_top=False, \n",
    "#     input_shape=(img_height, img_height, 3)\n",
    "# ) \n",
    "\n",
    "# x=model_base.output\n",
    "\n",
    "# x= GlobalAveragePooling2D()(x)\n",
    "# x= BatchNormalization()(x)\n",
    "# x= Dropout(0.5)(x)\n",
    "# x= Dense(1024,activation='relu')(x) \n",
    "# x= Dense(512,activation='relu')(x) \n",
    "# x= BatchNormalization()(x)\n",
    "# x= Dropout(0.5)(x)\n",
    "\n",
    "# preds=Dense(NUM_CLASSES,activation='softmax')(x) #FC-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"DenseNet121\"\n",
    "# optimizer = \"Adam\"\n",
    "\n",
    "# model_base=DenseNet121(\n",
    "#     include_top=False, \n",
    "#     input_shape=(img_height, img_height, 3)\n",
    "# ) \n",
    "\n",
    "# x=model_base.output\n",
    "# x= layers.Lambda(lambda x, y: (x ,tf.one_hot(y, depth=NUM_CLASSES)))\n",
    "# x= GlobalAveragePooling2D()(x)\n",
    "# x= Flatten()\n",
    "# x= Dense(256,activation='relu')\n",
    "# x= Dropout(0.7)(x)\n",
    "# x= BatchNormalization()(x)\n",
    "# x= Dense(128,activation='relu')(x) \n",
    "# x= Dropout(0.5)(x)\n",
    "# x= BatchNormalization()(x)\n",
    "# x= Dense(64,activation='relu')(x) \n",
    "# x= Dropout(0.3)(x)\n",
    "# x= BatchNormalization()(x)\n",
    "\n",
    "# preds=Dense(NUM_CLASSES,activation='softmax')(x) #FC-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"DenseNet121\"\n",
    "optimizer = \"Adam\"\n",
    "\n",
    "OldModel=DenseNet121(\n",
    "    include_top=False, \n",
    "    input_shape=(img_height, img_height, 3)\n",
    ") \n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(layers.Lambda(lambda x:K.backend.resize_images(x,height_factor=4,width_factor=4,data_format='channels_last')))\n",
    "model.add(OldModel)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.7))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# preds=Dense(NUM_CLASSES,activation='softmax')(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=Model(inputs=model_base.input,outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:-NUM_CLASSES]:\n",
    "    layer.trainable=False\n",
    "    \n",
    "for layer in model.layers[-NUM_CLASSES:]:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " densenet121 (Functional)    (None, 7, 7, 1024)        7037504   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 50176)             0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 50176)            200704    \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               12845312  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,126,468\n",
      "Trainable params: 8,772\n",
      "Non-trainable params: 20,117,696\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_callbacks = [\n",
    "#     tf.keras.callbacks.EarlyStopping(patience=2),\n",
    "#     tf.keras.callbacks.ModelCheckpoint(filepath='/ch/model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "#     tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learningRate = 0.0001\n",
    "# anne = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, verbose=1, min_lr=learningRate)\n",
    "# checkpoint = ModelCheckpoint('model.h5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2022-11-25 20:24:42.997176\n",
      "date and time = 20l24l42\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "print(\"now =\", now)\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "str_time = now.strftime(\"%Hl%Ml%S\")\n",
    "print(\"date and time =\", str_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.9335 - accuracy: 0.2533\n",
      "Epoch 1: val_loss improved from inf to 1.29930, saving model to ./ModelSave\\modelBest-20l24l42.h5\n",
      "105/105 [==============================] - 69s 459ms/step - loss: 1.9335 - accuracy: 0.2533 - val_loss: 1.2993 - val_accuracy: 0.4262 - lr: 1.0000e-04\n",
      "Epoch 2/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.7056 - accuracy: 0.3068\n",
      "Epoch 2: val_loss improved from 1.29930 to 1.16652, saving model to ./ModelSave\\modelBest-20l24l42.h5\n",
      "105/105 [==============================] - 42s 397ms/step - loss: 1.7056 - accuracy: 0.3068 - val_loss: 1.1665 - val_accuracy: 0.5917 - lr: 1.0000e-04\n",
      "Epoch 3/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.5568 - accuracy: 0.3631\n",
      "Epoch 3: val_loss improved from 1.16652 to 1.09108, saving model to ./ModelSave\\modelBest-20l24l42.h5\n",
      "105/105 [==============================] - 28s 267ms/step - loss: 1.5568 - accuracy: 0.3631 - val_loss: 1.0911 - val_accuracy: 0.5952 - lr: 1.0000e-04\n",
      "Epoch 4/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.4263 - accuracy: 0.4068\n",
      "Epoch 4: val_loss improved from 1.09108 to 1.05171, saving model to ./ModelSave\\modelBest-20l24l42.h5\n",
      "105/105 [==============================] - 44s 412ms/step - loss: 1.4263 - accuracy: 0.4068 - val_loss: 1.0517 - val_accuracy: 0.5964 - lr: 1.0000e-04\n",
      "Epoch 5/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.3461 - accuracy: 0.4533\n",
      "Epoch 5: val_loss improved from 1.05171 to 1.02886, saving model to ./ModelSave\\modelBest-20l24l42.h5\n",
      "105/105 [==============================] - 37s 347ms/step - loss: 1.3461 - accuracy: 0.4533 - val_loss: 1.0289 - val_accuracy: 0.5905 - lr: 1.0000e-04\n",
      "Epoch 6/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.3421 - accuracy: 0.4646\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 6: val_loss improved from 1.02886 to 1.01604, saving model to ./ModelSave\\modelBest-20l24l42.h5\n",
      "105/105 [==============================] - 30s 279ms/step - loss: 1.3421 - accuracy: 0.4646 - val_loss: 1.0160 - val_accuracy: 0.5929 - lr: 1.0000e-04\n",
      "Epoch 7/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.3123 - accuracy: 0.4801\n",
      "Epoch 7: val_loss improved from 1.01604 to 1.01086, saving model to ./ModelSave\\modelBest-20l24l42.h5\n",
      "105/105 [==============================] - 29s 269ms/step - loss: 1.3123 - accuracy: 0.4801 - val_loss: 1.0109 - val_accuracy: 0.5929 - lr: 5.0000e-05\n",
      "Epoch 8/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2750 - accuracy: 0.4857\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 8: val_loss improved from 1.01086 to 1.00695, saving model to ./ModelSave\\modelBest-20l24l42.h5\n",
      "105/105 [==============================] - 40s 379ms/step - loss: 1.2750 - accuracy: 0.4857 - val_loss: 1.0070 - val_accuracy: 0.5929 - lr: 5.0000e-05\n",
      "Epoch 9/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2715 - accuracy: 0.4988\n",
      "Epoch 9: val_loss improved from 1.00695 to 1.00487, saving model to ./ModelSave\\modelBest-20l24l42.h5\n",
      "105/105 [==============================] - 44s 415ms/step - loss: 1.2715 - accuracy: 0.4988 - val_loss: 1.0049 - val_accuracy: 0.5940 - lr: 2.5000e-05\n",
      "Epoch 10/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2537 - accuracy: 0.4949\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 10: val_loss improved from 1.00487 to 1.00362, saving model to ./ModelSave\\modelBest-20l24l42.h5\n",
      "105/105 [==============================] - 31s 286ms/step - loss: 1.2537 - accuracy: 0.4949 - val_loss: 1.0036 - val_accuracy: 0.5929 - lr: 2.5000e-05\n",
      "Epoch 11/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2596 - accuracy: 0.4943\n",
      "Epoch 11: val_loss improved from 1.00362 to 1.00291, saving model to ./ModelSave\\modelBest-20l24l42.h5\n",
      "105/105 [==============================] - 34s 268ms/step - loss: 1.2596 - accuracy: 0.4943 - val_loss: 1.0029 - val_accuracy: 0.5940 - lr: 1.2500e-05\n",
      "Epoch 12/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2588 - accuracy: 0.4940\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\n",
      "Epoch 12: val_loss improved from 1.00291 to 1.00200, saving model to ./ModelSave\\modelBest-20l24l42.h5\n",
      "105/105 [==============================] - 29s 268ms/step - loss: 1.2588 - accuracy: 0.4940 - val_loss: 1.0020 - val_accuracy: 0.5940 - lr: 1.2500e-05\n",
      "Epoch 13/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2342 - accuracy: 0.4994\n",
      "Epoch 13: val_loss improved from 1.00200 to 1.00109, saving model to ./ModelSave\\modelBest-20l24l42.h5\n",
      "105/105 [==============================] - 37s 285ms/step - loss: 1.2342 - accuracy: 0.4994 - val_loss: 1.0011 - val_accuracy: 0.5952 - lr: 6.2500e-06\n",
      "Epoch 14/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2435 - accuracy: 0.4979\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.00109\n",
      "105/105 [==============================] - 36s 268ms/step - loss: 1.2435 - accuracy: 0.4979 - val_loss: 1.0022 - val_accuracy: 0.5952 - lr: 6.2500e-06\n",
      "Epoch 15/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2533 - accuracy: 0.5012\n",
      "Epoch 15: val_loss improved from 1.00109 to 1.00094, saving model to ./ModelSave\\modelBest-20l24l42.h5\n",
      "105/105 [==============================] - 31s 294ms/step - loss: 1.2533 - accuracy: 0.5012 - val_loss: 1.0009 - val_accuracy: 0.5952 - lr: 3.1250e-06\n",
      "Epoch 16/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2617 - accuracy: 0.4991\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "\n",
      "Epoch 16: val_loss improved from 1.00094 to 0.99986, saving model to ./ModelSave\\modelBest-20l24l42.h5\n",
      "105/105 [==============================] - 34s 311ms/step - loss: 1.2617 - accuracy: 0.4991 - val_loss: 0.9999 - val_accuracy: 0.5952 - lr: 3.1250e-06\n",
      "Epoch 17/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2614 - accuracy: 0.4949\n",
      "Epoch 17: val_loss did not improve from 0.99986\n",
      "105/105 [==============================] - 30s 280ms/step - loss: 1.2614 - accuracy: 0.4949 - val_loss: 1.0006 - val_accuracy: 0.5952 - lr: 1.5625e-06\n",
      "Epoch 18/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2414 - accuracy: 0.5030\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.99986\n",
      "105/105 [==============================] - 29s 268ms/step - loss: 1.2414 - accuracy: 0.5030 - val_loss: 1.0010 - val_accuracy: 0.5952 - lr: 1.5625e-06\n",
      "Epoch 19/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2351 - accuracy: 0.5027\n",
      "Epoch 19: val_loss did not improve from 0.99986\n",
      "105/105 [==============================] - 29s 269ms/step - loss: 1.2351 - accuracy: 0.5027 - val_loss: 1.0006 - val_accuracy: 0.5940 - lr: 7.8125e-07\n",
      "Epoch 20/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2409 - accuracy: 0.5000\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.99986\n",
      "105/105 [==============================] - 29s 269ms/step - loss: 1.2409 - accuracy: 0.5000 - val_loss: 1.0006 - val_accuracy: 0.5940 - lr: 7.8125e-07\n",
      "Epoch 21/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2342 - accuracy: 0.4946\n",
      "Epoch 21: val_loss did not improve from 0.99986\n",
      "105/105 [==============================] - 29s 268ms/step - loss: 1.2342 - accuracy: 0.4946 - val_loss: 1.0007 - val_accuracy: 0.5952 - lr: 3.9062e-07\n",
      "Epoch 22/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2405 - accuracy: 0.4970\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.99986\n",
      "105/105 [==============================] - 29s 270ms/step - loss: 1.2405 - accuracy: 0.4970 - val_loss: 1.0002 - val_accuracy: 0.5952 - lr: 3.9062e-07\n",
      "Epoch 23/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2582 - accuracy: 0.4902\n",
      "Epoch 23: val_loss improved from 0.99986 to 0.99986, saving model to ./ModelSave\\modelBest-20l24l42.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 30s 283ms/step - loss: 1.2582 - accuracy: 0.4902 - val_loss: 0.9999 - val_accuracy: 0.5940 - lr: 1.9531e-07\n",
      "Epoch 24/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2616 - accuracy: 0.4848\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.99986\n",
      "105/105 [==============================] - 29s 270ms/step - loss: 1.2616 - accuracy: 0.4848 - val_loss: 1.0003 - val_accuracy: 0.5952 - lr: 1.9531e-07\n",
      "Epoch 25/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2440 - accuracy: 0.5080\n",
      "Epoch 25: val_loss did not improve from 0.99986\n",
      "105/105 [==============================] - 31s 288ms/step - loss: 1.2440 - accuracy: 0.5080 - val_loss: 0.9999 - val_accuracy: 0.5952 - lr: 9.7656e-08\n",
      "Epoch 26/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2452 - accuracy: 0.5015\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.99986\n",
      "105/105 [==============================] - 31s 286ms/step - loss: 1.2452 - accuracy: 0.5015 - val_loss: 1.0010 - val_accuracy: 0.5952 - lr: 9.7656e-08\n",
      "Epoch 27/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2091 - accuracy: 0.5161\n",
      "Epoch 27: val_loss did not improve from 0.99986\n",
      "105/105 [==============================] - 31s 293ms/step - loss: 1.2091 - accuracy: 0.5161 - val_loss: 1.0014 - val_accuracy: 0.5952 - lr: 4.8828e-08\n",
      "Epoch 28/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2502 - accuracy: 0.5030\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.99986\n",
      "105/105 [==============================] - 30s 285ms/step - loss: 1.2502 - accuracy: 0.5030 - val_loss: 1.0005 - val_accuracy: 0.5952 - lr: 4.8828e-08\n",
      "Epoch 29/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2492 - accuracy: 0.5000\n",
      "Epoch 29: val_loss did not improve from 0.99986\n",
      "105/105 [==============================] - 32s 299ms/step - loss: 1.2492 - accuracy: 0.5000 - val_loss: 1.0000 - val_accuracy: 0.5952 - lr: 2.4414e-08\n",
      "Epoch 30/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2501 - accuracy: 0.4991\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.99986\n",
      "105/105 [==============================] - 30s 282ms/step - loss: 1.2501 - accuracy: 0.4991 - val_loss: 1.0001 - val_accuracy: 0.5952 - lr: 2.4414e-08\n",
      "Epoch 31/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2318 - accuracy: 0.5214\n",
      "Epoch 31: val_loss did not improve from 0.99986\n",
      "105/105 [==============================] - 32s 296ms/step - loss: 1.2318 - accuracy: 0.5214 - val_loss: 1.0002 - val_accuracy: 0.5952 - lr: 1.2207e-08\n",
      "Epoch 32/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2379 - accuracy: 0.4940\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
      "\n",
      "Epoch 32: val_loss improved from 0.99986 to 0.99957, saving model to ./ModelSave\\modelBest-20l24l42.h5\n",
      "105/105 [==============================] - 33s 305ms/step - loss: 1.2379 - accuracy: 0.4940 - val_loss: 0.9996 - val_accuracy: 0.5952 - lr: 1.2207e-08\n",
      "Epoch 33/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2556 - accuracy: 0.4955\n",
      "Epoch 33: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 31s 287ms/step - loss: 1.2556 - accuracy: 0.4955 - val_loss: 1.0007 - val_accuracy: 0.5952 - lr: 6.1035e-09\n",
      "Epoch 34/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2580 - accuracy: 0.5024\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 30s 279ms/step - loss: 1.2580 - accuracy: 0.5024 - val_loss: 1.0014 - val_accuracy: 0.5940 - lr: 6.1035e-09\n",
      "Epoch 35/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2273 - accuracy: 0.5110\n",
      "Epoch 35: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 31s 290ms/step - loss: 1.2273 - accuracy: 0.5110 - val_loss: 1.0008 - val_accuracy: 0.5952 - lr: 3.0518e-09\n",
      "Epoch 36/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2173 - accuracy: 0.4988\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 31s 286ms/step - loss: 1.2173 - accuracy: 0.4988 - val_loss: 1.0013 - val_accuracy: 0.5952 - lr: 3.0518e-09\n",
      "Epoch 37/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2451 - accuracy: 0.5092\n",
      "Epoch 37: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 30s 277ms/step - loss: 1.2451 - accuracy: 0.5092 - val_loss: 1.0007 - val_accuracy: 0.5952 - lr: 1.5259e-09\n",
      "Epoch 38/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2437 - accuracy: 0.4955\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 28s 263ms/step - loss: 1.2437 - accuracy: 0.4955 - val_loss: 1.0010 - val_accuracy: 0.5952 - lr: 1.5259e-09\n",
      "Epoch 39/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2486 - accuracy: 0.5015\n",
      "Epoch 39: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 29s 266ms/step - loss: 1.2486 - accuracy: 0.5015 - val_loss: 1.0008 - val_accuracy: 0.5952 - lr: 7.6294e-10\n",
      "Epoch 40/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2338 - accuracy: 0.4940\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 28s 261ms/step - loss: 1.2338 - accuracy: 0.4940 - val_loss: 0.9999 - val_accuracy: 0.5952 - lr: 7.6294e-10\n",
      "Epoch 41/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2349 - accuracy: 0.4994\n",
      "Epoch 41: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 27s 252ms/step - loss: 1.2349 - accuracy: 0.4994 - val_loss: 1.0007 - val_accuracy: 0.5952 - lr: 3.8147e-10\n",
      "Epoch 42/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2398 - accuracy: 0.5003\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-10.\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 27s 251ms/step - loss: 1.2398 - accuracy: 0.5003 - val_loss: 1.0011 - val_accuracy: 0.5940 - lr: 3.8147e-10\n",
      "Epoch 43/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2383 - accuracy: 0.5030\n",
      "Epoch 43: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 27s 253ms/step - loss: 1.2383 - accuracy: 0.5030 - val_loss: 1.0012 - val_accuracy: 0.5940 - lr: 1.9073e-10\n",
      "Epoch 44/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2328 - accuracy: 0.5065\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-11.\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 27s 250ms/step - loss: 1.2328 - accuracy: 0.5065 - val_loss: 1.0012 - val_accuracy: 0.5952 - lr: 1.9073e-10\n",
      "Epoch 45/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2504 - accuracy: 0.5057\n",
      "Epoch 45: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 27s 250ms/step - loss: 1.2504 - accuracy: 0.5057 - val_loss: 1.0005 - val_accuracy: 0.5952 - lr: 9.5367e-11\n",
      "Epoch 46/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2486 - accuracy: 0.4997\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 4.768371461572052e-11.\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 27s 251ms/step - loss: 1.2486 - accuracy: 0.4997 - val_loss: 1.0008 - val_accuracy: 0.5952 - lr: 9.5367e-11\n",
      "Epoch 47/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2415 - accuracy: 0.4938\n",
      "Epoch 47: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 28s 258ms/step - loss: 1.2415 - accuracy: 0.4938 - val_loss: 1.0010 - val_accuracy: 0.5952 - lr: 4.7684e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2391 - accuracy: 0.5077\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 2.384185730786026e-11.\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 27s 253ms/step - loss: 1.2391 - accuracy: 0.5077 - val_loss: 1.0015 - val_accuracy: 0.5952 - lr: 4.7684e-11\n",
      "Epoch 49/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2412 - accuracy: 0.5024\n",
      "Epoch 49: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 28s 260ms/step - loss: 1.2412 - accuracy: 0.5024 - val_loss: 1.0010 - val_accuracy: 0.5952 - lr: 2.3842e-11\n",
      "Epoch 50/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2365 - accuracy: 0.5104\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 1.192092865393013e-11.\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 29s 275ms/step - loss: 1.2365 - accuracy: 0.5104 - val_loss: 1.0005 - val_accuracy: 0.5952 - lr: 2.3842e-11\n",
      "Epoch 51/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2504 - accuracy: 0.5021\n",
      "Epoch 51: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 30s 280ms/step - loss: 1.2504 - accuracy: 0.5021 - val_loss: 1.0005 - val_accuracy: 0.5952 - lr: 1.1921e-11\n",
      "Epoch 52/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2605 - accuracy: 0.4991\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 5.960464326965065e-12.\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 29s 271ms/step - loss: 1.2605 - accuracy: 0.4991 - val_loss: 1.0006 - val_accuracy: 0.5940 - lr: 1.1921e-11\n",
      "Epoch 53/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2381 - accuracy: 0.5039\n",
      "Epoch 53: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 30s 280ms/step - loss: 1.2381 - accuracy: 0.5039 - val_loss: 0.9999 - val_accuracy: 0.5952 - lr: 5.9605e-12\n",
      "Epoch 54/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2221 - accuracy: 0.5045\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 2.9802321634825324e-12.\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 31s 285ms/step - loss: 1.2221 - accuracy: 0.5045 - val_loss: 1.0004 - val_accuracy: 0.5952 - lr: 5.9605e-12\n",
      "Epoch 55/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2111 - accuracy: 0.5098\n",
      "Epoch 55: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 30s 280ms/step - loss: 1.2111 - accuracy: 0.5098 - val_loss: 1.0009 - val_accuracy: 0.5952 - lr: 2.9802e-12\n",
      "Epoch 56/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2352 - accuracy: 0.4926\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 1.4901160817412662e-12.\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 27s 253ms/step - loss: 1.2352 - accuracy: 0.4926 - val_loss: 1.0008 - val_accuracy: 0.5952 - lr: 2.9802e-12\n",
      "Epoch 57/120\n",
      "105/105 [==============================] - ETA: 0s - loss: 1.2813 - accuracy: 0.4848\n",
      "Epoch 57: val_loss did not improve from 0.99957\n",
      "105/105 [==============================] - 30s 285ms/step - loss: 1.2813 - accuracy: 0.4848 - val_loss: 1.0016 - val_accuracy: 0.5952 - lr: 1.4901e-12\n",
      "Epoch 58/120\n",
      " 47/105 [============>.................] - ETA: 13s - loss: 1.2492 - accuracy: 0.4887"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MinlearningRateAdj = 0.001\n",
    "nb_epochs = 120\n",
    "steps_per_epoch = np.ceil(len(train_ds) / batch_size)\n",
    "lr = 0.0001\n",
    "\n",
    "anne = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                         factor=0.5, \n",
    "                         patience=2, \n",
    "                         min_delta=0.001,\n",
    "                         verbose=1, \n",
    "#                          min_lr=MinlearningRateAdj, \n",
    "                        \n",
    "                        )\n",
    "CALLBACKS.append(K.callbacks.ModelCheckpoint(\n",
    "    filepath=r'./ModelSave/modelEnd-'+str_time+'.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True))\n",
    "checkpoint = ModelCheckpoint(r'./ModelSave/modelBest-'+str_time+'.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=nb_epochs,\n",
    "    verbose=1,\n",
    "#     callbacks=[checkpoint]\n",
    "    callbacks=[anne, checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r'./ModelSave/modelEnd-'+str_time+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(nb_epochs)\n",
    "\n",
    "print(\"Model = \"+model_name)\n",
    "print(\"DataName = \"+Dataname)\n",
    "print(\"Optimizer = \"+optimizer)\n",
    "print(\"Epochs = {}\".format(nb_epochs))\n",
    "print(\"Image Size = {}\".format(img_width))\n",
    "print(\"Batch = {}\".format(batch_size))\n",
    "print(\"learningRate = {}\".format(lr))\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "#####  use this  to save train graph as png #####################\n",
    "from datetime import datetime\n",
    "dt = datetime.now()\n",
    "ts = datetime.timestamp(dt)\n",
    "date_time = datetime.fromtimestamp(ts)\n",
    "str_date_time = date_time.strftime(\"%d-%m-%Y_%H-%M\")\n",
    "graph_path = r'C:\\Users\\thanaphat turienngam\\Desktop\\Cassava\\classification\\experimentData\\graph'\n",
    "if( not (os.path.exists(graph_path)) ) :\n",
    "    os.mkdir(graph_path)\n",
    "plt.savefig(graph_path+'/'+str_date_time+\".png\")  \n",
    "#####  use this  to save train graph as png #####################\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "graph_path = graph_path+'/'+str_date_time+\".png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = keras.models.load_model(r\"C:\\Users\\Chale\\Desktop\\AI WORK\\CODE\\cassava_model\\model\\DenseNet121noaug94.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# img_path = DATASET_PATH+r\"\\test\\cmd\\test-img-43.jpg\"\n",
    "# img = tf.keras.utils.load_img(\n",
    "#     img_path, target_size=(img_height, img_width)\n",
    "# )\n",
    "# img_array = tf.keras.utils.img_to_array(img)\n",
    "# img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "# predictions = model.predict(img_array)\n",
    "# score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "# print(\n",
    "#     \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "#     .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir =r\"C:\\Users\\thanaphat turienngam\\Desktop\\Cassava\\classification\\dataset_aug\\trainwithaugment\\test\"\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=1,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_ds.map(tf.autograph.experimental.do_not_convert(lambda x, y: (x , \n",
    "                                      tf.one_hot(y, depth=NUM_CLASSES))))\n",
    "result = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = []\n",
    "test_y = []\n",
    "for images, labels in test_data:\n",
    "    test_x.append(images)\n",
    "    test_y.append(np.argmax(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "preds = []\n",
    "i=0;\n",
    "for img in test_x:\n",
    "    preds.append(np.argmax(model.predict(img, verbose=0)))\n",
    "    i=i+1\n",
    "    print(\"Running : \"+str(i)+\"/\"+str(len(test_x))+\" = \"+str(i/len(test_x)*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(preds)\n",
    "test_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(preds)\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = test_ds.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confm = tf.math.confusion_matrix(\n",
    "    test_y,\n",
    "    preds,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    dtype=tf.dtypes.int32,\n",
    "    name=class_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confm = np.array(confm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "df_cm = pd.DataFrame((confm/confm.astype(np.float32).sum(axis=1)), index =class_names,\n",
    "              columns =class_names )\n",
    "plt.figure(figsize = (12,9))\n",
    "plt.title('All fold confusion matrix of the multi-class classification\\n\\nModel '+model_name+'\\n\\nMean Accuracy is {:.2f}'.format((np.mean(result[1]))))\n",
    "sns.heatmap(df_cm, annot=True,cmap=\"Blues\",fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
